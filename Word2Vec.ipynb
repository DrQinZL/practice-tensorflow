{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from embedding import gen_word2vec as word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-7041ef759ebc>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-7041ef759ebc>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print tmp\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# practice tf.reduce_sum, tf.matmul\n",
    "\n",
    "tmp = [[1,2,1],[2,3,1]]\n",
    "\n",
    "print tmp\n",
    "print tf.reduce_sum(tmp).eval()\n",
    "print tf.reduce_sum(tmp, 0).eval()\n",
    "print tf.reduce_sum(tmp, 1).eval()\n",
    "print \"=\"*30\n",
    "\n",
    "a = np.ones([2,4])\n",
    "b = np.ones([2,4])\n",
    "print tf.matmul(a, b, transpose_b=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_dim = 200\n",
    "num_neg_samples = 100\n",
    "\n",
    "epochs_to_train = 5\n",
    "learning_rate = 0.025\n",
    "batch_size = 16\n",
    "window_size = 5\n",
    "min_count = 5\n",
    "subsample = 1e-3\n",
    "\n",
    "# words : list of words including 'UNK'\n",
    "# counts : list of counts of words\n",
    "# words_per_epoch\n",
    "# current_epoch : start from 0\n",
    "(words, counts, words_per_epoch, current_epoch, total_words_processed, x, y) = \\\n",
    "    word2vec.skipgram(\n",
    "        filename = 'text8',\n",
    "        batch_size = batch_size,\n",
    "        window_size = window_size,\n",
    "        min_count = min_count,\n",
    "        subsample = subsample\n",
    "    )\n",
    "\n",
    "(vocab_words, vocab_counts, words_per_epoch) = sess.run([words, counts, words_per_epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  71290  + UNK\n",
      "Words per epoch:  17005207\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_words)\n",
    "\n",
    "id2word = vocab_words\n",
    "word2id = {}\n",
    "\n",
    "for i, w in enumerate(id2word):\n",
    "    word2id[w] = i\n",
    "\n",
    "print(\"Vocab size: \", vocab_size - 1, \" + UNK\")\n",
    "print(\"Words per epoch: \", words_per_epoch)\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3134 3134 3134 3134 3134 3134 3134 3134   46   46   59   59   59   59   59\n",
      "   59]\n",
      "[  742   477   195     2  3134    46    59   128   742   477 10573   134\n",
      "  3134    46    59   156]\n"
     ]
    }
   ],
   "source": [
    "print(x.eval())\n",
    "print(y.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w_in : vocab_size x embed_dim\n",
    "embed = tf.Variable(\n",
    "    tf.random_uniform(\n",
    "        [vocab_size, embed_dim],\n",
    "        -0.5 / embed_dim,\n",
    "        +0.5 / embed_dim\n",
    "    ),\n",
    "    name = \"w_in\"\n",
    ")\n",
    "\n",
    "# w_out : vocab_size x embed_dim\n",
    "w = tf.Variable(tf.zeros([vocab_size, embed_dim]), name=\"w\")\n",
    "b = tf.Variable(tf.zeros([vocab_size]), name=\"b\")\n",
    "\n",
    "# Cast [batch_size] -> [batch_size x 1]\n",
    "labels_matrix = tf.reshape(tf.cast(y, dtype=tf.int64), [batch_size, 1])\n",
    "\n",
    "# Samples a set of classes using the provided (fixed) base distribution.\n",
    "neg_sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "    true_classes = labels_matrix,\n",
    "    num_true = 1,\n",
    "    num_sampled = num_neg_samples,\n",
    "    unique = True,\n",
    "    range_max = vocab_size,\n",
    "    distortion = 0.75,\n",
    "    unigrams = vocab_counts.tolist()))\n",
    "\n",
    "# batch_size x embed_dim\n",
    "x_emb = tf.nn.embedding_lookup(embed, x)\n",
    "\n",
    "# batch_size x embed_dim\n",
    "true_w = tf.nn.embedding_lookup(w, y)\n",
    "\n",
    "# batch_size x 1\n",
    "true_b = tf.nn.embedding_lookup(b, y)\n",
    "\n",
    "# element-wise multiplication\n",
    "# tf.reduced_sum([batch_size x embed_dim] * [batch_size x embed_dim]) + [batch_size x 1]\n",
    "# tf.reduce_sum(x, 1) : row끼리 더함\n",
    "true_y_ = tf.reduce_sum(tf.mul(x_emb, true_w), 1) + true_b\n",
    "\n",
    "\n",
    "########################\n",
    "# For negative samples\n",
    "########################\n",
    "\n",
    "# num_neg_samples x embed_dim\n",
    "neg_w = tf.nn.embedding_lookup(w, neg_sampled_ids)\n",
    "\n",
    "# num_neg_samples x 1\n",
    "neg_b = tf.nn.embedding_lookup(b, neg_sampled_ids)\n",
    "neg_b = tf.reshape(neg_b, [num_neg_samples])\n",
    "\n",
    "# matrix multiplication\n",
    "# [batch_size x embed_dim] x [embed_dim x num_neg_samples] + [num_neg_samples] <- 각 row 마다 neg_b 더함\n",
    "# neg_y_ = tf.reduce_sum(tf.mul(x_emb, ))\n",
    "neg_y_ = tf.matmul(x_emb, neg_w, transpose_b=True) + neg_b\n",
    "\n",
    "# neg_y_ 계산이 이상해 보이지만 true_y_ 구할 때 처럼 element-wise multiplication해서 neg_b 더하는것\n",
    "\n",
    "#####################################\n",
    "# Negative Cross Entropy (NCE) Loss\n",
    "#####################################\n",
    "\n",
    "# Measures the probability error in discrete classification tasks in which each\n",
    "# class is independent and not mutually exclusive. For instance, one could\n",
    "# perform multilabel classification where a picture can contain both an elephant\n",
    "# and a dog at the same time.\n",
    "# >>> tmp = tf.maximum(y, 0) - y * y_ + tf.log(tf.ones_like(y) + tf.exp(-y))\n",
    "# >>> [[ 0.69314718  0.7132616   0.69314718]]\n",
    "# Range from 0 to 1\n",
    "true_loss = tf.nn.sigmoid_cross_entropy_with_logits(true_y_, tf.ones_like(true_y_))\n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(neg_y_, tf.zeros_like(neg_y_))\n",
    "\n",
    "loss = (tf.reduce_sum(true_loss) + tf.reduce_sum(neg_loss)) / batch_size\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Optimization\n",
    "#####################################\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "words_to_train = float(words_per_epoch * epochs_to_train)\n",
    "\n",
    "lr = learning_rate * tf.maximum(\n",
    "    0.001,\n",
    "    1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train\n",
    ")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "train = optimizer.minimize(loss,\n",
    "                           global_step = global_step,\n",
    "                           gate_gradients=optimizer.GATE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b8f8d53f3e29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcurrent_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_thread_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-b8f8d53f3e29>\u001b[0m in \u001b[0;36mtrain_thread_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 458\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "concurrent_steps = 12\n",
    "statistics_interval = 5\n",
    "checkpoint_interval = 600\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with open(os.path.join(\"./\", \"vocab.txt\"), \"w\") as f:\n",
    "    for i in xrange(vocab_size):\n",
    "        f.write(vocab_words[i] + \" \" + str(vocab_counts[i]) + \"\\n\")\n",
    "\n",
    "def train_thread_body():\n",
    "    global sess, train, current_epoch\n",
    "    initial_epoch = sess.run([current_epoch])\n",
    "    \n",
    "    while True:\n",
    "        _, epoch = sess.run([train, current_epoch])\n",
    "        if epoch != initial_epoch:\n",
    "            break\n",
    "\n",
    "###############\n",
    "# Train\n",
    "###############\n",
    "\n",
    "for _ in xrange(epochs_to_train):\n",
    "    initial_epoch, initial_words = sess.run([current_epoch, words])\n",
    "    \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    summary_writer = tf.train.SummaryWriter(\"./\",\n",
    "                                            graph_def=sess.graph_def)\n",
    "    \n",
    "    workers = []\n",
    "    for _ in xrange(concurrent_steps):\n",
    "        t = threading.Thread(target=train_thread_body())\n",
    "        t.start()\n",
    "        workers.append(t)\n",
    "        \n",
    "    last_words, last_tme, last_summary_time = initial_words, time.time(), 0\n",
    "    last_checkpoint_time = 0\n",
    "    \n",
    "    while True:\n",
    "        # Print statistics every n seconds.\n",
    "        time.sleep(statistics_interval)\n",
    "        (epoch, step, loss, words, lr) = sess.run(\n",
    "              [current_epoch, global_step, loss, words, lr])\n",
    "        \n",
    "        now = time.time()\n",
    "        last_words, last_time, rate = words,now, (words - last_words) / (now - last_time)\n",
    "        print(\"Epoch %4d Step %8d: lr = %5.3f loss = %6.2f words/sec = %8.0f\\r\" %\n",
    "            (epoch, step, lr, loss, rate), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if now - last_checkpoint_time > checkpoint_interval:\n",
    "            saver.save(sess, \"./\" + \"model\", global_step = step.astype(int))\n",
    "            last_checkpoint_time = now\n",
    "            \n",
    "        if epoch != initial_epoch:\n",
    "            break\n",
    "            \n",
    "    for t in workers:\n",
    "        t.join()\n",
    "\n",
    "# global_step : to distinguish step from other saved models\n",
    "saver.save(sess, os.path.join(\"./\", \"model.ckpt\"), global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.ones([3., 3.])\n",
    "tmp[1][1] = 0\n",
    "tmp[2][1] = 0\n",
    "tmp[2][2] = 0\n",
    "\n",
    "print(tmp)\n",
    "print(tf.nn.l2_normalize(tmp.astype(\"float32\"), 1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Graph for evaluation\n",
    "########################\n",
    "\n",
    "analogy_a = tf.placeholder(dtype=tf.int32) # [N]\n",
    "analogy_b = tf.placeholder(dtype=tf.int32) # [N]\n",
    "analogy_c = tf.placeholder(dtype=tf.int32) # [N]\n",
    "\n",
    "# [vocab_size x embed_dim]\n",
    "normalized_embed = tf.nn.l2_normalize(embed, 1)\n",
    "\n",
    "# [N x embed_dim]\n",
    "a_embed = tf.gather(normalized_embed, analogy_a)\n",
    "b_embed = tf.gather(normalized_embed, analogy_b)\n",
    "c_embed = tf.gather(normalized_embed, analogy_c)\n",
    "\n",
    "target = c_embed + (b_embed - a_embed)\n",
    "\n",
    "# reference http://stats.stackexchange.com/questions/146221/is-cosine-similarity-identical-to-l2-normalized-euclidean-distance\n",
    "\n",
    "# cosine(A,B) : angle between vector A and B (1: same, -1: totally different)\n",
    "# Compute cosine distance between each pair of target and vocab.\n",
    "# [N x embed_dim] * [embed_dim x vocab_size]  = [N x vocab_size]\n",
    "dist = tf.matmul(target, normalized_embed, transpose_b = True)\n",
    "\n",
    "_, pred_idx = tf.nn.top_k(dist, 4)\n",
    "\n",
    "\n",
    "# Nodes for computing neighbors for a given word according to their cosine distance.\n",
    "# [N]\n",
    "nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# [N x embed_dim]\n",
    "nearby_embed = tf.gather(normalized_embed, nearby_word)\n",
    "\n",
    "# [N x embed_dim] x [embed_dim x vocab_size] = [N x vocab_size]\n",
    "nearby_dist = tf.matmul(nearby_emb, normalized_embed, transpose_b=True)\n",
    "\n",
    "# Calculate top 1000 nearby index for each words\n",
    "nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n",
    "                                     min(1000, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.top_k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed = tf.Variable(\n",
    "    tf.random_uniform(\n",
    "        [5, 5],\n",
    "        -0.5 / 5,\n",
    "        +0.5 / 5\n",
    "    ),\n",
    "    name = \"w_in\"\n",
    ")\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "print(embed.eval())\n",
    "print(tf.nn.embedding_lookup(embed, [2,1,4]).eval())\n",
    "print(tf.gather(embed, [2,1,4]).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tf.nn.embedding_lookup(params, ids)\n",
    "\n",
    "Looks up `ids` in a list of embedding tensors.\n",
    "\n",
    "This function is used to perform parallel lookups on the list of\n",
    "tensors in `params`.  It is a generalization of\n",
    "[`tf.gather()`](../../api_docs/python/array_ops.md#gather), where `params` is\n",
    "interpreted as a partition of a larger embedding tensor.\n",
    "\n",
    "If `len(params) > 1`, each element `id` of `ids` is partitioned between\n",
    "the elements of `params` by computing `p = id % len(params)`, and is\n",
    "then used to look up the slice `params[p][id // len(params), ...]`.\n",
    "\n",
    "The results of the lookup are then concatenated into a dense\n",
    "tensor. The returned tensor has shape `shape(ids) + shape(params)[1:]`.\n",
    "\n",
    "# tf.gather(params, indices)\n",
    "\n",
    "Gather slices from `params` according to `indices`.\n",
    "\n",
    "`indices` must be an integer tensor of any dimension (usually 0-D or 1-D).\n",
    "Produces an output tensor with shape `indices.shape + params.shape[1:]` where:\n",
    "\n",
    "    # Scalar indices\n",
    "    output[:, ..., :] = params[indices, :, ... :]\n",
    "\n",
    "    # Vector indices\n",
    "    output[i, :, ..., :] = params[indices[i], :, ... :]\n",
    "\n",
    "    # Higher rank indices\n",
    "    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n",
    "\n",
    "If `indices` is a permutation and `len(indices) == params.shape[0]` then\n",
    "this operation will permute `params` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = embed.eval()\n",
    "nembed = tf.nn.l2_normalize(embed, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n",
    "nearby_emb = tf.reshape(tf.gather(nembed, nearby_word), [1, embed_dim])\n",
    "nearby_dist = tf.matmul(nearby_emb, nembed, transpose_b=True)\n",
    "nearby_val, nearby_idx = tf.nn.top_k(nearby_dist, min(50, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['UNK', 'the', 'of', ..., 'jimsonweed', 'shinkansen', 'lophophore'], dtype=object),\n",
       " array([ 286363, 1061396,  593677, ...,       5,       5,       5], dtype=int32),\n",
       " 17005207)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words, vocab_counts, words_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearby(idx, num=20):\n",
    "    ids = np.array([x for x in idx])\n",
    "    vals, idx = sess.run(\n",
    "        [nearby_val, nearby_idx], {nearby_word: ids})\n",
    "    for i, id in enumerate(idx):\n",
    "        print(\"\\n%s\\n=====================================\" % (vocab_words[id]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (vocab_words[neighbor], distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['two' 'three' 'four' 'six' 'zero' 'one' 'five' 'eight' 'seven' 'june'\n",
      " 'april' 'billion' 'january' 'february' 'march' 'july' 'december' 'mm'\n",
      " 'august' 'nine' 'september' 'kg' 'km' 'november' 'bwv' 'october' 'ft' 'mi'\n",
      " 'million' 'lb' 'retrieved' 'cm' 'inches' 'miles' 'sq' 'summer' 'unpaved'\n",
      " 'pp' 'imports' 'nautical' 'paved' 'accessed' 'feet' 'mhz' 'nm' 'cubic'\n",
      " 'airports' 'finalist' 'square' 'quarter']\n",
      "=====================================\n",
      "two                  1.0000\n",
      "three                0.9548\n",
      "four                 0.9408\n",
      "six                  0.9262\n",
      "zero                 0.9261\n",
      "one                  0.9229\n",
      "five                 0.9229\n",
      "eight                0.9113\n",
      "seven                0.9078\n",
      "june                 0.8708\n",
      "april                0.8667\n",
      "billion              0.8658\n",
      "january              0.8655\n",
      "february             0.8644\n",
      "march                0.8630\n",
      "july                 0.8630\n",
      "december             0.8617\n",
      "mm                   0.8568\n",
      "august               0.8566\n",
      "nine                 0.8565\n"
     ]
    }
   ],
   "source": [
    "nearby([10], num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
