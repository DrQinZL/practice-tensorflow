{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f950ec9ded0>> ignored\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import nltk.data\n",
    "import collections\n",
    "from glob import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "punctuation_remover = RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "\n",
    "def stopword_filter(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "def read_name_data(data_dir):\n",
    "    with open(os.path.join(data_dir, 'name.txt')) as f:\n",
    "        name_lists = f.readlines()\n",
    "        \n",
    "    names = [name.lower().strip().split('\\t') for name in name_lists]\n",
    "\n",
    "    #name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "    #print(name_counter.most_common(100))\n",
    "\n",
    "    name2idx = {}\n",
    "    nameword2idx = {}\n",
    "\n",
    "    for idx, name_set in enumerate(names):\n",
    "        for name in name_set:\n",
    "            name2idx[name] = idx\n",
    "            word_in_name = name.split()\n",
    "            name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "            for name in [name, name_without_punctuation] + word_in_name:\n",
    "                try:\n",
    "                    if idx not in name_dict[name]:\n",
    "                        name2idx[name].append(idx)\n",
    "                except:\n",
    "                    nameword2idx[name] = idx\n",
    "\n",
    "    idx2name = dict(zip(name2idx.values(), name2idx.keys()))\n",
    "    \n",
    "    return names, name2idx, idx2name, nameword2idx\n",
    "\n",
    "def read_data_as_words(data_dir):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    text = \"\"\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' in filename:\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            text += f.read()\n",
    "    return text.split()\n",
    "\n",
    "def read_data_as_sentences(data_dir, nameword2idx):\n",
    "    sentences = []\n",
    "\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' in filename:\n",
    "            continue\n",
    "\n",
    "        with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "            nltk_splited_sentences = tokenizer.tokenize(stopword_filter(f.read().encode('ascii','ignore').lower()))\n",
    "            current_sentences = [\" \".join(punctuation_remover.tokenize(sentence)) for sentence in nltk_splited_sentences\n",
    "                                 if any(word in nameword2idx.keys() for word in sentence.split())]\n",
    "            sentences.extend(current_sentences)\n",
    "\n",
    "            print(\" [*] %s finished: %d / %d\" % (filename, len(current_sentences), len(nltk_splited_sentences)))\n",
    "\n",
    "    idx2sentences = {}\n",
    "    for sentence in sentences:\n",
    "        for idx in [idx for nameword, idx in nameword2idx.items() if nameword in sentence]:\n",
    "            idx2sentences.setdefault(idx, []).append(\" \".join([word for word in sentence.split() if word not in nameword2idx.keys()]))\n",
    "    \n",
    "    new_sentences = []\n",
    "    name_idx_of_sentence = []\n",
    "    for idx in idx2sentences.keys():\n",
    "        for sentence in idx2sentences[idx]:\n",
    "            new_sentences.append(sentence)\n",
    "            name_idx_of_sentence.append(idx)\n",
    "    \n",
    "    print(\" [*] Total sentences : %d\" % (len(sentences)))\n",
    "    return sentences, name_idx_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset_from_sentences(sentences, name_idx_of_sentence):\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    \n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    word2idx = dict()\n",
    "    for word, _ in count:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "    data = list()\n",
    "    label_data = list()\n",
    "    unk_count = 0\n",
    "    for sentence, name_idx in zip(sentences, name_idx_of_sentence):\n",
    "        for word in sentence.split():\n",
    "            if word in word2idx:\n",
    "                index = word2idx[word]\n",
    "            else:\n",
    "                index = 0\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "            label_data.append(name_idx)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "\n",
    "    return word2idx, idx2word, data, label_data, count\n",
    "\n",
    "def generate_batch(data, label_data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(batch_size):\n",
    "        batch[i] = data[data[data_index]]\n",
    "        labels[i] = label_data[label_data[data_index]]\n",
    "        \n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size : 1081571\n",
      "# of names : 189\n",
      " [*] ./data/1.txt finished: 2263 / 6358\n",
      " [*] ./data/6.txt finished: 4416 / 11431\n",
      " [*] ./data/2.txt finished: 2704 / 6496\n",
      " [*] ./data/5.txt finished: 6956 / 17300\n",
      " [*] ./data/7.txt finished: 4917 / 14304\n",
      " [*] ./data/4.txt finished: 5466 / 13804\n",
      " [*] ./data/3.txt finished: 3483 / 8660\n",
      " [*] Total sentences : 30205\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "words = read_data_as_words(data_dir)\n",
    "print('Data size :', len(words))\n",
    "\n",
    "names, name2idx, idx2name, nameword2idx = read_name_data(data_dir)\n",
    "print('# of names :', len(names))\n",
    "\n",
    "sentences, name_idx_of_sentence = read_data_as_sentences(data_dir, nameword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{25, 78, 101, 113, 179, 183}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unkown names : %s\" % (set(name2idx.values()) - set(name_idx_of_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) : [['UNK', 0], ('harry', 13177), ('said', 6757), ('ron', 4293), ('hermione', 3792)]\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, data, label_data, count = build_dataset_from_sentences(sentences, name_idx_of_sentence)\n",
    "print('Most common words (+UNK) :', count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "batch_size = 128\n",
    "skip_window = 4\n",
    "num_skips = 2\n",
    "\n",
    "embed_size = 200\n",
    "neg_sample_size = 64\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size])\n",
    "y = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "neg_y = tf.placeholder(tf.int32, [neg_sample_size])\n",
    "\n",
    "init_width = 0.5 / embed_size\n",
    "\n",
    "embed = tf.Variable(tf.random_uniform([len(idx2name), embed_size], -init_width, init_width))\n",
    "w = tf.Variable(tf.zeros([vocab_size, embed_size]))\n",
    "b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "pos_embed = tf.nn.embedding_lookup(embed, x) # [batch_size x embed_size]\n",
    "pos_w = tf.nn.embedding_lookup(w, y)         # [batch_size x embed_size]\n",
    "pos_b = tf.nn.embedding_lookup(b, y)         # [batch_size x 1]\n",
    "\n",
    "pos_y_ = tf.add(tf.reduce_sum(tf.mul(pos_embed, pos_w), 1), pos_b) # [batch_size]\n",
    "\n",
    "# neg_embed = pos_embed\n",
    "neg_w = tf.nn.embedding_lookup(w, neg_y)     # [neg_sample_size x embed_size]\n",
    "neg_b = tf.nn.embedding_lookup(b, neg_y)     # [neg_sample_size]\n",
    "\n",
    "neg_y_ = tf.matmul(pos_embed, neg_w, transpose_b=True) + neg_b # [batch_size x neg_sample_size]\n",
    "\n",
    "pos_y = tf.ones_like(pos_y_)\n",
    "neg_y = tf.ones_like(neg_y_)\n",
    "\n",
    "pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(pos_y_, tf.ones_like(pos_y_))\n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(neg_y_, tf.zeros_like(neg_y_))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(w, b, pos_embed, y, neg_sample_size, vocab_size)\n",
    ")\n",
    "\n",
    "##################\n",
    "# Optimizer\n",
    "##################\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "inc = global_step.assign_add(1)\n",
    "\n",
    "# total_word_processed = float(word_per_epoch * epochs_to_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "lr = learning_rate * tf.maximum(\n",
    "    0.001,\n",
    "    1.0 - tf.cast(global_step, tf.float32) / num_steps\n",
    ")\n",
    "\n",
    "# loss = (tf.reduce_sum(pos_loss) + tf.reduce_sum(neg_loss))/batch_size\n",
    "\n",
    "with tf.control_dependencies([inc]):\n",
    "    train = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Index 312 at offset 0 in Tindices is out of range\n\t [[Node: embedding_lookup_10 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_6, _recv_Placeholder_9_0)]]\nCaused by op u'embedding_lookup_10', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-139-3aad513e457d>\", line 19, in <module>\n    pos_embed = tf.nn.embedding_lookup(embed, x) # [batch_size x embed_size]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 50, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 302, in gather\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 639, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1757, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1008, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-7d9d80e37f6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[1;32m--> 474\u001b[1;33m                                               e.code)\n\u001b[0m\u001b[0;32m    475\u001b[0m         \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Index 312 at offset 0 in Tindices is out of range\n\t [[Node: embedding_lookup_10 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_6, _recv_Placeholder_9_0)]]\nCaused by op u'embedding_lookup_10', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-139-3aad513e457d>\", line 19, in <module>\n    pos_embed = tf.nn.embedding_lookup(embed, x) # [batch_size x embed_size]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.py\", line 50, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 302, in gather\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 639, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1757, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1008, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "average_loss = 0\n",
    "for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        data, label_data, batch_size, num_skips, skip_window\n",
    "    )\n",
    "    feed_dict = {x: batch_inputs, y: batch_labels}\n",
    "    _, loss_val = sess.run([train, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / 2000\n",
    "\n",
    "        print(\"Average loss at step \", step, \": \", average_loss)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"data_dir\", './data/', \"Directory which contains data files\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = FLAGS.data_dir\n",
    "\n",
    "def main():\n",
    "    if not FLAGS.data_dir:\n",
    "        print(\"--data_dir must be specified\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    opts = Options()\n",
    "    read_data(opts.data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
