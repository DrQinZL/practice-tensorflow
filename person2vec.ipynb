{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import nltk.data\n",
    "import collections\n",
    "from glob import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "punctuation_remover = RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nameword2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ecc8179c6eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnameword2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nameword2idx' is not defined"
     ]
    }
   ],
   "source": [
    "with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "    sentences = tokenizer.tokenize(f.read())\n",
    "\n",
    "sentences[1]\n",
    "nameword2idx.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f28191e9dcd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mnameword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0midx2name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('./data', 'name.txt')) as f:\n",
    "    name_lists = f.readlines()\n",
    "\n",
    "names = [name.strip().split('\\t') for name in name_lists]\n",
    "\n",
    "#name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "#print(name_counter.most_common(100))\n",
    "\n",
    "name2idx = {}\n",
    "nameword2idx = {}\n",
    "\n",
    "for idx, name_set in enumerate(names):\n",
    "    for name in name_set:\n",
    "        word_in_name = name.split()\n",
    "        name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "        for name in [name, name_without_punctuation]:\n",
    "            try:\n",
    "                if idx not in name_dict[name]:\n",
    "                    name2idx[name].append(idx)\n",
    "            except:\n",
    "                name2idx[name] = []\n",
    "                name2idx[name].append(idx)\n",
    "        \n",
    "        for name in word_in_name:\n",
    "            try:\n",
    "                if idx not in name_dict[name]:\n",
    "                    nameword2idx[name].append(idx)\n",
    "            except:\n",
    "                nameword2idx[name] = []\n",
    "                nameword2idx[name].append(idx)\n",
    "\n",
    "idx2name = dict(zip(name2idx.values(), name2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stopword_filter(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hogwarts\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "\n",
    "def read_name_data(data_dir):\n",
    "    with open(os.path.join(data_dir, 'name.txt')) as f:\n",
    "        name_lists = f.readlines()\n",
    "        \n",
    "    names = [name.lower().strip().split('\\t') for name in name_lists]\n",
    "\n",
    "    #name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "    #print(name_counter.most_common(100))\n",
    "\n",
    "    name2idx = {}\n",
    "    nameword2idx = {}\n",
    "\n",
    "    for idx, name_set in enumerate(names):\n",
    "        for name in name_set:\n",
    "            name2idx[name] = idx\n",
    "            word_in_name = name.split()\n",
    "            name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "            for name in [name, name_without_punctuation] + word_in_name:\n",
    "                try:\n",
    "                    if idx not in name_dict[name]:\n",
    "                        name2idx[name].append(idx)\n",
    "                except:\n",
    "                    nameword2idx[name] = []\n",
    "                    nameword2idx[name].append(idx)\n",
    "\n",
    "    idx2name = dict(zip(name2idx.values(), name2idx.keys()))\n",
    "    \n",
    "    return names, name2idx, idx2name, nameword2idx\n",
    "\n",
    "def read_data(data_dir):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    text = \"\"\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' not in filename:\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            text += f.read()\n",
    "    tokenizer.tokenize(text)\n",
    "    return text.split()\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    return dictionary, reverse_dictionary, data, count\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window    # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size : 396\n",
      "# of names : 189\n",
      "Most common words (+UNK) : [['UNK', 0], ('Weasley', 9), ('Potter', 6), ('Longbottom', 4), ('Dursley', 4)]\n"
     ]
    }
   ],
   "source": [
    "words = read_data('./data/')\n",
    "print('Data size :', len(words))\n",
    "\n",
    "names, name2idx, idx2name, nameword2idx = read_name_data(\"./data/\")\n",
    "print('# of names :', len(names))\n",
    "\n",
    "dictionary, reverse_dictionary, data, count = build_dataset(words)\n",
    "print('Most common words (+UNK) :', count[:5])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] ./data/1.txt finished: 2110 / 4861\n",
      " [*] ./data/6.txt finished: 3984 / 8235\n",
      " [*] ./data/2.txt finished: 2527 / 5148\n",
      " [*] ./data/5.txt finished: 6316 / 12451\n",
      " [*] ./data/7.txt finished: 4510 / 10492\n",
      " [*] ./data/4.txt finished: 5034 / 10519\n",
      " [*] ./data/3.txt finished: 3232 / 6614\n",
      " [*] Total sentences : 27706\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for filename in glob(os.path.join('./data', \"*.txt\")):\n",
    "    if 'name.txt' in filename:\n",
    "        continue\n",
    "    with open(filename, 'r') as f:\n",
    "        nltk_splited_sentences = tokenizer.tokenize(stopword_filter(f.read().decode('utf-8').lower()))\n",
    "        current_sentences = [sentence for sentence in nltk_splited_sentences\n",
    "                             if any(word in nameword2idx.keys() for word in sentence.split())]\n",
    "        sentences.extend(current_sentences[1:])\n",
    "        \n",
    "        print(\" [*] %s finished: %d / %d\" % (filename, len(current_sentences), len(nltk_splited_sentences)))\n",
    "\n",
    "print(\" [*] Total sentences : %d\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry potter sorcerer’s stone j. k. rowling say you’ve spent first 10 years life sleeping stairs family loathes you. then, absurd, magical twist fate find surrounded wizards, caged snowy owl, phoenix-feather wand, jellybeans come every flavor, including strawberry, curry, grass, sardine. that, discover wizard yourself! exactly happens young harry potter j. k. rowling’s enchanting, funny debut novel, harry potter sorcerer’s stone. nonmagic human world—the world “muggles”—harry nobody, treated like dirt aunt uncle begrudgingly inherited parents killed evil voldemort. world wizards, small, skinny harry famous survivor wizard tried kill him. left lightning-bolt scar forehead, curiously refined sensibilities, host mysterious powers remind he’s quite, yes, altogether different aunt, uncle, spoiled, piglike cousin dudley. mysterious letter, delivered friendly giant hagrid, wrenches harry dreary, muggle-ridden existence: “we pleased inform accepted hogwarts school witchcraft wizardry.” course,\n"
     ]
    }
   ],
   "source": [
    "print(stopword_filter(open('./data/1.txt').read().decode('utf-8').lower())[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exactly happens young harry potter j. k. rowling’s enchanting, funny debut novel, harry potter sorcerer’s stone.\n",
      "world wizards, small, skinny harry famous survivor wizard tried kill him.\n",
      "mysterious letter, delivered friendly giant hagrid, wrenches harry dreary, muggle-ridden existence: “we pleased inform accepted hogwarts school witchcraft wizardry.” course, uncle vernon yells unpleasantly, “i paying crackpot old fool teach magic tricks!” soon enough, however, harry finds hogwarts owl hedwig… that’s real adventure—humorous, haunting, suspenseful—begins.\n",
      "harry potter sorcerer’s stone, first published england harry potter philosopher’s stone, continues win major awards england.\n",
      "1. boy lived mr. mrs. dursley, number four, privet drive, proud say perfectly normal, thank much.\n"
     ]
    }
   ],
   "source": [
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [sentence for sentence in tokenizer.tokenize(codecs.open('./data/1.txt', 'r', 'utf-8').read())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.ones([3,2,1]))\n",
    "print(tf.reduce_sum(np.ones([3,2,1]),0).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "b = tf.Variable(np.array([[0], [1], [2], [3], [4], [5]]), name=\"b\")\n",
    "embed = tf.nn.embedding_lookup(b, y)\n",
    "reshape = tf.reshape(embed, [3])\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "print(embed.eval({y:[1,4,1]}))\n",
    "print(reshape.eval({y:[1,4,1]}))\n",
    "print(tf.add(tf.zeros([vocab_size]).eval(),1).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_factory(vocab_size):\n",
    "    embed_size = 200\n",
    "    batch_size = 16\n",
    "    neg_sample_size = 6\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size])\n",
    "    y = tf.placeholder(tf.int32, [batch_size])\n",
    "    neg_y = tf.placeholder(tf.int32, [neg_sample_size])\n",
    "\n",
    "    init_width = 0.5 / embed_size\n",
    "\n",
    "    embed = tf.Variable(tf.random_uniform([vocab_size, embed_size], -init_width, init_width))\n",
    "    w = tf.Variable(tf.zeros([vocab_size, embed_size]))\n",
    "    b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    pos_embed = tf.nn.embedding_lookup(embed, x) # [batch_size x embed_size]\n",
    "    pos_w = tf.nn.embedding_lookup(w, y)         # [batch_size x embed_size]\n",
    "    pos_b = tf.nn.embedding_lookup(b, y)         # [batch_size x 1]\n",
    "\n",
    "    pos_y_ = tf.add(tf.reduce_sum(tf.mul(pos_embed, pos_w), 1), pos_b) # [batch_size]\n",
    "\n",
    "    # neg_embed = pos_embed\n",
    "    neg_w = tf.nn.embedding_lookup(w, neg_y)     # [neg_sample_size x embed_size]\n",
    "    neg_b = tf.nn.embedding_lookup(b, neg_y)     # [neg_sample_size]\n",
    "\n",
    "    neg_y_ = tf.matmul(pos_embed, neg_w, transpose_b=True) + neg_b # [batch_size x neg_sample_size]\n",
    "\n",
    "    pos_y = tf.ones_like(pos_y_)\n",
    "    neg_y = tf.ones_like(neg_y_)\n",
    "\n",
    "    pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(pos_y_, tf.ones_like(pos_y_))\n",
    "    neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(neg_y_, tf.zeros_like(neg_y_))\n",
    "\n",
    "    ##################\n",
    "    # Optimizer\n",
    "    ##################\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "    total_word_processed = float(word_per_epoch * epochs_to_train)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    lr = learning_rate * tf.maximum(\n",
    "        0.001,\n",
    "        1.0 - tf.cast(total_word_processed, tf.float32) / words_to_train\n",
    "    )\n",
    "\n",
    "    loss = (tf.reduce_sum(pos_loss) + tf.reduce_sum(neg_loss))/batch_size\n",
    "    train = tf.train.GradientDescentOptimizer(lr).minimize(loss,\n",
    "                                                           global_step = global_step,\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"data_dir\", './data/', \"Directory which contains data files\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = FLAGS.data_dir\n",
    "\n",
    "def main():\n",
    "    if not FLAGS.data_dir:\n",
    "        print(\"--data_dir must be specified\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    opts = Options()\n",
    "    read_data(opts.data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
