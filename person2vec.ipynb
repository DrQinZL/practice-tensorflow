{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f950ec9ded0>> ignored\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import nltk.data\n",
    "import collections\n",
    "from glob import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "punctuation_remover = RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "\n",
    "def stopword_filter(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "def read_name_data(data_dir):\n",
    "    with open(os.path.join(data_dir, 'name.txt')) as f:\n",
    "        name_lists = f.readlines()\n",
    "        \n",
    "    names = [name.lower().strip().split('\\t') for name in name_lists]\n",
    "\n",
    "    #name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "    #print(name_counter.most_common(100))\n",
    "\n",
    "    name2idx = {}\n",
    "    nameword2idx = {}\n",
    "\n",
    "    for idx, name_set in enumerate(names):\n",
    "        for name in name_set:\n",
    "            name2idx[name] = idx\n",
    "            word_in_name = name.split()\n",
    "            name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "            for name in [name, name_without_punctuation] + word_in_name:\n",
    "                try:\n",
    "                    if idx not in name_dict[name]:\n",
    "                        name2idx[name].append(idx)\n",
    "                except:\n",
    "                    nameword2idx[name] = idx\n",
    "\n",
    "    idx2name = dict(zip(name2idx.values(), name2idx.keys()))\n",
    "    \n",
    "    return names, name2idx, idx2name, nameword2idx\n",
    "\n",
    "def read_data_as_words(data_dir):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    text = \"\"\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' in filename:\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            text += f.read()\n",
    "    return text.split()\n",
    "\n",
    "def read_data_as_sentences(data_dir, nameword2idx):\n",
    "    sentences = []\n",
    "\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' in filename:\n",
    "            continue\n",
    "\n",
    "        with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "            nltk_splited_sentences = tokenizer.tokenize(stopword_filter(f.read().encode('ascii','ignore').lower()))\n",
    "            current_sentences = [\" \".join(punctuation_remover.tokenize(sentence)) for sentence in nltk_splited_sentences\n",
    "                                 if any(word in nameword2idx.keys() for word in sentence.split())]\n",
    "            sentences.extend(current_sentences)\n",
    "\n",
    "            print(\" [*] %s finished: %d / %d\" % (filename, len(current_sentences), len(nltk_splited_sentences)))\n",
    "\n",
    "    idx2sentences = {}\n",
    "    for sentence in sentences:\n",
    "        for idx in [idx for nameword, idx in nameword2idx.items() if nameword in sentence]:\n",
    "            idx2sentences.setdefault(idx, []).append(\" \".join([word for word in sentence.split() if word not in nameword2idx.keys()]))\n",
    "    \n",
    "    new_sentences = []\n",
    "    name_idx_of_sentence = []\n",
    "    for idx in idx2sentences.keys():\n",
    "        for sentence in idx2sentences[idx]:\n",
    "            new_sentences.append(sentence)\n",
    "            name_idx_of_sentence.append(idx)\n",
    "    \n",
    "    print(\" [*] Total sentences : %d\" % (len(sentences)))\n",
    "    return sentences, name_idx_of_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset_from_sentences(sentences, name_idx_of_sentence):\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    \n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    word2idx = dict()\n",
    "    for word, _ in count:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "    data = list()\n",
    "    label_data = list()\n",
    "    unk_count = 0\n",
    "    for sentence, name_idx in zip(sentences, name_idx_of_sentence):\n",
    "        for word in sentence.split():\n",
    "            if word in word2idx:\n",
    "                index = word2idx[word]\n",
    "            else:\n",
    "                index = 0\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "            label_data.append(name_idx)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "\n",
    "    return word2idx, idx2word, data, label_data, count\n",
    "\n",
    "def generate_batch(data, label_data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(batch_size):\n",
    "        batch[i] = data[data_index]\n",
    "        labels[i] = label_data[data_index]\n",
    "        \n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size : 1081571\n",
      "# of names : 189\n",
      " [*] ./data/1.txt finished: 2263 / 6358\n",
      " [*] ./data/6.txt finished: 4416 / 11431\n",
      " [*] ./data/2.txt finished: 2704 / 6496\n",
      " [*] ./data/5.txt finished: 6956 / 17300\n",
      " [*] ./data/7.txt finished: 4917 / 14304\n",
      " [*] ./data/4.txt finished: 5466 / 13804\n",
      " [*] ./data/3.txt finished: 3483 / 8660\n",
      " [*] Total sentences : 30205\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "words = read_data_as_words(data_dir)\n",
    "print('Data size :', len(words))\n",
    "\n",
    "names, name2idx, idx2name, nameword2idx = read_name_data(data_dir)\n",
    "print('# of names :', len(names))\n",
    "\n",
    "sentences, name_idx_of_sentence = read_data_as_sentences(data_dir, nameword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unkown names : set([101, 78, 113, 179, 183, 25])\n"
     ]
    }
   ],
   "source": [
    "print(\"Unkown names : %s\" % (set(name2idx.values()) - set(name_idx_of_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) : [['UNK', 0], ('harry', 13177), ('said', 6757), ('ron', 4293), ('hermione', 3792)]\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, data, label_data, count = build_dataset_from_sentences(sentences, name_idx_of_sentence)\n",
    "print('Most common words (+UNK) :', count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "batch_size = 128\n",
    "skip_window = 4\n",
    "num_skips = 2\n",
    "\n",
    "embed_size = 200\n",
    "neg_sample_size = 64\n",
    "\n",
    "x_ = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "x = tf.reshape(x_, [batch_size])\n",
    "y_ = tf.placeholder(tf.int32, [batch_size])\n",
    "y = tf.reshape(y_, [batch_size, 1])\n",
    "neg_y = tf.placeholder(tf.int32, [neg_sample_size])\n",
    "\n",
    "init_width = 0.5 / embed_size\n",
    "\n",
    "embed = tf.Variable(tf.random_uniform([len(idx2name), embed_size], -init_width, init_width))\n",
    "w = tf.Variable(tf.zeros([vocab_size, embed_size]))\n",
    "b = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "pos_embed = tf.nn.embedding_lookup(embed, x, name=\"pos_embed\") # [batch_size x embed_size]\n",
    "# pos_w = tf.nn.embedding_lookup(w, y, name=\"pos_w\")             # [batch_size x embed_size]\n",
    "# pos_b = tf.nn.embedding_lookup(b, y, name=\"pos_b\")             # [batch_size x 1]\n",
    "\n",
    "# pos_y_ = tf.add(tf.reduce_sum(tf.mul(pos_embed, pos_w), 1), pos_b) # [batch_size]\n",
    "\n",
    "# # neg_embed = pos_embed\n",
    "# neg_w = tf.nn.embedding_lookup(w, neg_y, name=\"neg_w\") # [neg_sample_size x embed_size]\n",
    "# neg_b = tf.nn.embedding_lookup(b, neg_y, name=\"neg_b\") # [neg_sample_size]\n",
    "\n",
    "# neg_y_ = tf.matmul(pos_embed, neg_w, transpose_b=True) + neg_b # [batch_size x neg_sample_size]\n",
    "\n",
    "# pos_y = tf.ones_like(pos_y_)\n",
    "# neg_y = tf.ones_like(neg_y_)\n",
    "\n",
    "# pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(pos_y_, tf.ones_like(pos_y_))\n",
    "# neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(neg_y_, tf.zeros_like(neg_y_))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(w, b, pos_embed, y, neg_sample_size, vocab_size)\n",
    ")\n",
    "\n",
    "##################\n",
    "# Optimizer\n",
    "##################\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\")\n",
    "inc = global_step.assign_add(1)\n",
    "\n",
    "# total_word_processed = float(word_per_epoch * epochs_to_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "lr = learning_rate * tf.maximum(\n",
    "    0.001,\n",
    "    1.0 - tf.cast(global_step, tf.float32) / num_steps\n",
    ")\n",
    "\n",
    "# loss = (tf.reduce_sum(pos_loss) + tf.reduce_sum(neg_loss))/batch_size\n",
    "\n",
    "with tf.control_dependencies([inc]):\n",
    "    train = tf.train.GradientDescentOptimizer(lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  272.591552734\n",
      "Average loss at step  2000 :  228.609821514\n",
      "Average loss at step  4000 :  130.241226244\n",
      "Average loss at step  6000 :  92.223043869\n",
      "Average loss at step  8000 :  82.0557265139\n",
      "Average loss at step  10000 :  61.8222225347\n",
      "Average loss at step  12000 :  46.7285290828\n",
      "Average loss at step "
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "average_loss = 0\n",
    "for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        data, label_data, batch_size, num_skips, skip_window\n",
    "    )\n",
    "    feed_dict = {x_: batch_labels, y_: batch_inputs}\n",
    "    _, loss_val = sess.run([train, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / 2000\n",
    "\n",
    "        print(\"Average loss at step \", step, \": \", average_loss)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"data_dir\", './data/', \"Directory which contains data files\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = FLAGS.data_dir\n",
    "\n",
    "def main():\n",
    "    if not FLAGS.data_dir:\n",
    "        print(\"--data_dir must be specified\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    opts = Options()\n",
    "    read_data(opts.data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
