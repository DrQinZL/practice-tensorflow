{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as np\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import nltk.data\n",
    "import collections\n",
    "from glob import glob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "punctuation_remover = RegexpTokenizer(r'\\w+')\n",
    "stopwords = nltk_stopwords.words('english')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for filename in glob(os.path.join('./data', \"*.txt\")):\n",
    "    if 'name.txt' in filename:\n",
    "        names = read_name_file(filename)\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        sentences.extend([sentence for sentence in tokenizer.tokenize(f.read())\n",
    "                          if any(word in nameword2idx.keys() for word in sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "    sentences = tokenizer.tokenize(f.read())\n",
    "\n",
    "sentences[1]\n",
    "nameword2idx.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('./data', 'name.txt')) as f:\n",
    "    name_lists = f.readlines()\n",
    "\n",
    "names = [name.strip().split('\\t') for name in name_lists]\n",
    "\n",
    "#name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "#print(name_counter.most_common(100))\n",
    "\n",
    "name2idx = {}\n",
    "nameword2idx = {}\n",
    "\n",
    "for idx, name_set in enumerate(names):\n",
    "    for name in name_set:\n",
    "        word_in_name = name.split()\n",
    "        name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "        for name in [name, name_without_punctuation]:\n",
    "            try:\n",
    "                if idx not in name_dict[name]:\n",
    "                    name2idx[name].append(idx)\n",
    "            except:\n",
    "                name2idx[name] = []\n",
    "                name2idx[name].append(idx)\n",
    "        \n",
    "        for name in word_in_name:\n",
    "            try:\n",
    "                if idx not in name_dict[name]:\n",
    "                    nameword2idx[name].append(idx)\n",
    "            except:\n",
    "                nameword2idx[name] = []\n",
    "                nameword2idx[name].append(idx)\n",
    "\n",
    "idx2name = dict(zip(name2idx.values(), name2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zacharias', 'Smith']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stopword_filter(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_name_data(data_dir):\n",
    "    with open(os.path.join(data_dir, 'name.txt')) as f:\n",
    "        name_lists = f.readlines()\n",
    "        \n",
    "    names = [name.strip().split('\\t') for name in name_lists]\n",
    "\n",
    "    #name_counter = collections.Counter([word for name_set in names for name in name_set for word in name.split()])\n",
    "    #print(name_counter.most_common(100))\n",
    "\n",
    "    name2idx = {}\n",
    "    nameword2idx = {}\n",
    "\n",
    "    for idx, name_set in enumerate(names):\n",
    "        for name in name_set:\n",
    "            name2idx[name] = idx\n",
    "            word_in_name = name.split()\n",
    "            name_without_punctuation = \" \".join(punctuation_remover.tokenize(name))\n",
    "\n",
    "            for name in [name, name_without_punctuation] + word_in_name:\n",
    "                try:\n",
    "                    if idx not in name_dict[name]:\n",
    "                        name2idx[name].append(idx)\n",
    "                except:\n",
    "                    nameword2idx[name] = []\n",
    "                    nameword2idx[name].append(idx)\n",
    "\n",
    "    idx2name = dict(zip(name2idx.values(), name2idx.keys()))\n",
    "    \n",
    "    return names, name2idx, idx2name, nameword2idx\n",
    "\n",
    "def read_data(data_dir):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    text = \"\"\n",
    "    for filename in glob(os.path.join(data_dir, \"*.txt\")):\n",
    "        if 'name.txt' not in filename:\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            text += f.read()\n",
    "    tokenizer.tokenize(text)\n",
    "    return text.split()\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    return dictionary, reverse_dictionary, data, count\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window    # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size : 396\n",
      "# of names : 189\n",
      "Most common words (+UNK) : [['UNK', 0], ('Weasley', 9), ('Potter', 6), ('Longbottom', 4), ('Dursley', 4)]\n"
     ]
    }
   ],
   "source": [
    "words = read_data('./data/')\n",
    "print('Data size :', len(words))\n",
    "\n",
    "names, name2idx, idx2name, nameword2idx = read_name_data(\"./data/\")\n",
    "print('# of names :', len(names))\n",
    "\n",
    "dictionary, reverse_dictionary, data, count = build_dataset(words)\n",
    "print('Most common words (+UNK) :', count[:5])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"data_dir\", './data/', \"Directory which contains data files\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self):\n",
    "        self.data_dir = FLAGS.data_dir\n",
    "\n",
    "def main():\n",
    "    if not FLAGS.data_dir:\n",
    "        print(\"--data_dir must be specified\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    opts = Options()\n",
    "    read_data(opts.data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
